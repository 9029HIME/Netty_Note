# 粘包与Netty的Decoder

## 概述

对于Netty的使用，往往集中在TCP协议，TCP协议作为流式协议，无可避免会有粘包的问题。对于粘包问题，主流有以下几种解决方案：

1. 短连接：一个请求建立一个连接，太过笨重，不考虑。
2. 固定长度：将消息按照 固定长度 进行分割，对于不足的部分采取Padding补齐操作。这种方式比较简单，但浪费空间。
3. 分隔符：对消息进行读取，以 自定义分隔符 为界限，界定消息的完整性、独立性。空间使用率高，相对简单，就是分隔符作为内容的话需要转移，并且需要逐个逐个读取，效率偏低。
4. TLV：将消息以 Tag、Length、Value 的格式进行打包发送，接收者也按照 Tag、Lenght、Value 的方式进行解析。空间利用率、读取效率最高，但是开发成本也比较高，对于消息长度有 提前约定的限制，比较适用的方案。

对于Netty来说，数据解码和编码是通过Encoder、Decoder完成，对于上面的方案，Netty也有现成的类提供支持：

1. 固定长度：FixedLengthFrameDecoder，至于编码，在发消息的时候简单处理一下就好了。
2. 分隔符：DelimiterBasedFrameDecoder，至于编码，在发消息的时候简单处理一下就好了。
3. TLV：LengthFieldBasedFameDecoder、LengthFieldPrepender（Encoder）。

## ByteToMessageDecoder

不管使用哪种Decoder，都是继承自`ByteToMessageDecoder`。首先要明白：既然Netty需要解码，说明数据源自可读事件；既然是可读事件，Decoder最终是在Pipeline靠前的部分对数据进行读操作；既然是读操作，还是会回到channelRead方法里：

```java
// io.netty.handler.codec.ByteToMessageDecoder#channelRead
public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
    if (msg instanceof ByteBuf) {
        CodecOutputList out = CodecOutputList.newInstance();
        boolean var10 = false;
        try {
            var10 = true;
            ByteBuf data = (ByteBuf)msg;
            this.first = this.cumulation == null;
            if (this.first) {
                this.cumulation = data; // 如果是第一读，直接将数据扔给cumulation，它是Netty对SocketChannel可读数据的积累（后面会讲到）。
            } else {
                // 如果第n次读，则将数据追加到这个SocketChannel的cumulation里。
                this.cumulation = this.cumulator.cumulate(ctx.alloc(), this.cumulation, data);
            }
            this.callDecode(ctx, this.cumulation, out); // 实际的解码逻辑，交给对应的Decoder实现类处理。
            var10 = false;  
            // 省略
        }
        // 省略
    }
    // 省略
}


// io.netty.handler.codec.ByteToMessageDecoder#callDecode
protected void callDecode(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) {
        try {
            while(true) {
                if (in.isReadable()) {
                    int outSize = out.size();
                    if (outSize > 0) {
                        fireChannelRead(ctx, out, outSize);
                        out.clear();
                        if (ctx.isRemoved()) {
                            return;
                        }
                        outSize = 0;
                    }
                    int oldInputLength = in.readableBytes();
                    this.decodeRemovalReentryProtection(ctx, in, out); // 对cumulation的可读数据进行decode，decode完成后在cumulation清掉已读数据。
                    // 省略
                }
                return;
            }
        }
    	// 省略
    }
}


// io.netty.handler.codec.ByteToMessageDecoder#decodeRemovalReentryProtection
final void decodeRemovalReentryProtection(ChannelHandlerContext ctx, ByteBuf in, List<Object> out) throws Exception {
        this.decodeState = 1;
        boolean var8 = false;

        try {
            var8 = true;
            this.decode(ctx, in, out); // 利用模板模式，调用具体实现类的decode方法，完成解码。
            var8 = false;
        } finally {
            if (var8) {
                boolean removePending = this.decodeState == 2;
                this.decodeState = 0;
                if (removePending) {
                    this.handlerRemoved(ctx);
                }

            }
        }

        boolean removePending = this.decodeState == 2;
        this.decodeState = 0;
        if (removePending) {
            this.handlerRemoved(ctx);
        }

    }
```

# Netty的探活

## TCP的Keepalive

TCP协议本身就有心跳检测机制，在Linux Kernel有三个设置影响到Keepalive的行为：

1. tcp_keepalive_time 7200 距离上次传送数据多少时间未收到新报文判断为开始检测，单位秒，默认7200s(没必要频繁，浪费资源)。
2. tcp_keepalive_intvl 75// 检测开始每多少时间发送心跳包，单位秒，默认75s。
3. tcp_keepalive_probes 9// 发送几次心跳包对方未响应则close连接，默认9次。

## 为什么还需要应用层的Keepalive

对于TCP来说，Keepalive只能检测连接是否存活，只要另一端保持心跳通信，TCP就认为这个连接是可用的。然而，对于应用层来说，这种探活方式不一定适用于业务需求，具体的应用会有具体的探活原则。简单来说：TCP只关心**连接是否断连**，而应用层更应该关心**连接在业务上还能不能用**。

## Keepalive的优化：Idle检测

对于Netty的Keepalive，如果采用定时发送心跳的方式，会比较耗费资源。想一想，对于一个Server服务器，如果承载着百万连接，在交换业务数据报文的同时，每一个连接都要定时发一下心跳，这样对性能也是有影响的，**我们更希望将这些浪费的性能用在业务报文的传输上**，因此可以使用Idle对心跳进行优化。

使用Idle的核心逻辑是：Client与Server在正常传输数据的时候，不发送Keepalive报文。当超过一段时间没有发送报文，判定这个连接为Idle状态，并对连接发送Keepalive进行探活。

## Idle的使用

之前的笔记已经讲过了，这里做一次复习：

首先Idle状态有READER_IDLE、WRITER_IDLE、ALL_IDLE，分别代表 **这个SocketChannel** 没数据可读、没数据可写、没有数据读并没有数据写。

对于SocketChannel的pipeline，需要添加一个IdleStateHandler对象，它的构造函数有4个参数构成，分别是：READER_IDLE时间阈值、WRITER_IDLE时间阈值、ALL_IDLE时间阈值、时间阈值单位。

这个IdleStateHandler对象内置了一个TimeoutTask的定时任务，扫描这个SocketChannel是否命中Idle，如果命中的话，将IdleState传给pipeline下一个Handler的userEventTriggered方法进行处理，这个Handler可以是自定义的，也可以使用Netty提供的。

如下面代码所示，这个SocketChannel如果3秒内没有读操作、5秒内没有写操作、7秒内没有读和写操作，就会交给HeartBeatHandler进行处理，打印出日志。

```java
public class InitHandler extends ChannelInitializer<SocketChannel> {
    @Override
    protected void initChannel(SocketChannel ch) throws Exception {
        ch.pipeline()
        .addLast(new IdleStateHandler(3,5,7, TimeUnit.SECONDS))
        .addLast(new HeartBeatHandler());
    }
}


public class HeartBeatHandler extends ChannelInboundHandlerAdapter {

    @Override
    public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception {
        if(evt instanceof IdleStateEvent){
            IdleStateEvent event = (IdleStateEvent) evt;

            switch (event.state()){
                case READER_IDLE:
                    System.out.println(String.format("ip地址为%s的通道发生了读空闲",ctx.channel().remoteAddress()));
                    break;
                case WRITER_IDLE:
                    System.out.println(String.format("ip地址为%s的通道发生了写空闲",ctx.channel().remoteAddress()));
                    break;
                case ALL_IDLE:
                    System.out.println(String.format("ip地址为%s的通道发生了空闲",ctx.channel().remoteAddress()));
                    break;
            }
        }
    }
}

```

## Idle与关闭连接

值得注意的是，如果客户端手动关闭连接，会触发四次挥手，Netty可以接收到这个事件，并调用Handler的handlerRemove()和channelInactive()进行资源释放处理。但是，如果客户端强制关闭连接（如强制结束进程，或电脑强制关机），这时Netty接收不了四次挥手请求，因此才需要Idle机制，定期清除过期连接。